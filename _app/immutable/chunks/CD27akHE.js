import{t as $,b as a,a as o}from"./Cw4I5osi.js";import"./BzVk5r6l.js";import{c as he,s,f,n as r,r as $e}from"./NgVQVlRK.js";import{n as fe}from"./B4IyMRKX.js";import{H as g}from"./CXsRaEhZ.js";import"./hMT8fFzP.js";import"./NwRJ91PD.js";import{C as me,a as P}from"./rEuJ3T1U.js";/* empty css        */import"./BZUG4Puk.js";import{I as ve}from"./CmUbQthf.js";import"./D9bBQFLV.js";import{C as _e}from"./DXp9_3zM.js";import{F as le}from"./OFUKRh55.js";import{L as w,I as v}from"./BhmTgGWB.js";import{P as u}from"./D8YsId2T.js";import{L as _}from"./yh4_9ChP.js";import"./POtwinrL.js";import"./CbbZjpT6.js";/* empty css        *//* empty css        */import{P as ge}from"./CEkRzcqJ.js";const de={layout:"post",title:"State of audio processing",description:"An overview of the latest developments in audio processing with AI and their impact on various industries.",date:"2024-03-27T00:00:00.000Z",cover:"/images/blog/state-of-audio-processing/cover.png",timeToRead:15,author:"luke-silver",category:"product",featured:!1};var ye=$("AI in audio processing began in the mid-20th century with work on sound synthesis methods. <!> laid the groundwork for digital audio synthesis.",1),xe=$("In the late 1970s, the potential of AI in understanding and changing audio started to be explored. Early projects like the <!> demonstrated the possibilities of speech synthesis.",1),Pe=$("The 1990s saw the introduction of machine learning techniques like hidden Markov models (HMMs) for speech recognition. <!> was influential. HMMs had limitations, requiring substantial manual work and struggling with complex speech patterns.",1),we=$("The 2014 paper <!> showed the power of CNNs in learning from raw audio data, improving speech recognition performance. LSTMs, introduced by <!>, proved effective in handling temporal dependencies in audio.",1),be=$("These techniques enhanced speech recognition and enabled audio generation. Models like <!> generated lifelike speech and music by learning from large audio datasets.",1),ke=$("<!><!><!>",1),Ne=$("<!><!><!>",1),Ae=$("The model is trained on labeled data (recordings where we know what instrument is playing) using the <!> function. After training, it can predict the instrument for new recordings.",1),Te=$("<!><!><!><!>",1),Me=$("<!>, was a breakthrough in audio generation. It can generate realistic speech and musical sounds by predicting the next sample in an audio waveform based on the samples that came before it. <!> built upon WaveNet to create high-quality synthetic speech that&#39;s nearly indistinguishable from human speech.",1),De=$("For music composition, <!> and <!> models are important developments. MuseNet can generate 4-minute musical compositions with 10 different instruments, while Jukebox can generate music in the style of popular genres and artists. These models demonstrate the potential for AI to be a powerful tool for musicians and composers.",1),Se=$("<!><!><!>",1),qe=$("<article><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!><!></article>");function Ie(ue){ge(ue,fe(de,{children:(ce,ze)=>{var x=qe(),b=he(x);u(b,{children:(t,n)=>{r();var e=a("Audio processing is a rapidly advancing field within artificial intelligence where algorithms are designed to interpret and manipulate audio signals. From the early days of basic sound synthesis to today's voice assistants and automatic music generation, AI in audio processing has undergone significant development. Let's explore the history of this field and the key concepts and innovations that have shaped it.");o(t,e)},$$slots:{default:!0}});var k=s(b);g(k,{level:1,children:(t,n)=>{r();var e=a("A (not so) brief history of AI in audio processing");o(t,e)},$$slots:{default:!0}});var N=s(k);u(N,{children:(t,n)=>{r();var e=ye(),l=s(f(e));_(l,{href:"https://en.wikipedia.org/wiki/Max_Mathews",children:(c,p)=>{r();var i=a("Max Mathews at Bell Labs in the 1950s");o(c,i)},$$slots:{default:!0}}),r(),o(t,e)},$$slots:{default:!0}});var A=s(N);u(A,{children:(t,n)=>{r();var e=xe(),l=s(f(e));_(l,{href:"https://ieeexplore.ieee.org/document/1454357",children:(c,p)=>{r();var i=a("'Talking Typewriter' by Kurzweil and Shrobe in 1978");o(c,i)},$$slots:{default:!0}}),r(),o(t,e)},$$slots:{default:!0}});var T=s(A);u(T,{children:(t,n)=>{r();var e=Pe(),l=s(f(e));_(l,{href:"https://ieeexplore.ieee.org/document/18626",children:(c,p)=>{r();var i=a(`Rabiner's 1989 paper "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition"`);o(c,i)},$$slots:{default:!0}}),r(),o(t,e)},$$slots:{default:!0}});var M=s(T);g(M,{level:1,children:(t,n)=>{r();var e=a("The deep learning revolution");o(t,e)},$$slots:{default:!0}});var D=s(M);u(D,{children:(t,n)=>{r();var e=a("A major breakthrough in audio processing came with deep learning in the 2010s. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, revolutionized speech recognition.");o(t,e)},$$slots:{default:!0}});var S=s(D);u(S,{children:(t,n)=>{r();var e=we(),l=s(f(e));_(l,{href:"https://arxiv.org/abs/1412.5567",children:(p,i)=>{r();var d=a('"Deep Speech: Scaling up end-to-end speech recognition" by Hannun et al.');o(p,d)},$$slots:{default:!0}});var c=s(l,2);_(c,{href:"https://www.bioinf.jku.at/publications/older/2604.pdf",children:(p,i)=>{r();var d=a("Hochreiter and Schmidhuber in 1997");o(p,d)},$$slots:{default:!0}}),r(),o(t,e)},$$slots:{default:!0}});var q=s(S);u(q,{children:(t,n)=>{r();var e=be(),l=s(f(e));_(l,{href:"https://arxiv.org/abs/1609.03499",children:(c,p)=>{r();var i=a("WaveNet from DeepMind in 2016");o(c,i)},$$slots:{default:!0}}),r(),o(t,e)},$$slots:{default:!0}});var I=s(q);g(I,{level:1,children:(t,n)=>{r();var e=a("Key concepts in modern audio processing");o(t,e)},$$slots:{default:!0}});var z=s(I);g(z,{level:2,children:(t,n)=>{r();var e=a("Fourier transforms and spectral analysis");o(t,e)},$$slots:{default:!0}});var F=s(z);u(F,{children:(t,n)=>{r();var e=a("When you have a recording of a sound, it's a mix of many different frequencies (pitches) playing together. Fourier transforms are a way to separate these frequencies so you can see how much of each frequency is present in the sound.");o(t,e)},$$slots:{default:!0}});var C=s(F);u(C,{children:(t,n)=>{r();var e=a("This is useful because:");o(t,e)},$$slots:{default:!0}});var H=s(C);w(H,{ordered:!1,marker:"-",children:(t,n)=>{var e=ke(),l=f(e);v(l,{children:(i,d)=>{r();var h=a("You can see which notes are being played by different instruments");o(i,h)},$$slots:{default:!0}});var c=s(l);v(c,{children:(i,d)=>{r();var h=a("You can remove frequencies you don't want, like noise or certain instruments");o(i,h)},$$slots:{default:!0}});var p=s(c);v(p,{children:(i,d)=>{r();var h=a("You can change the volume of specific frequencies to alter the balance of the sound");o(i,h)},$$slots:{default:!0}}),o(t,e)},$$slots:{default:!0}});var R=s(H);u(R,{children:(t,n)=>{r();var e=a("Here's a simple code example:");o(t,e)},$$slots:{default:!0}});var L=s(R);le(L,{content:`import numpy as np

# Make a simple sound wave
t = np.linspace(0, 1, 500)  
f = 10  # Frequency in Hz
y = np.sin(2*np.pi*f*t)

# Apply Fourier transform
Y = np.fft.fft(y)

# Plot the frequency content
import matplotlib.pyplot as plt
plt.plot(np.abs(Y))
plt.show()
`,language:"python",process:!0,children:(t,n)=>{r();var e=a(`import numpy as np

# Make a simple sound wave
t = np.linspace(0, 1, 500)  
f = 10  # Frequency in Hz
y = np.sin(2*np.pi*f*t)

# Apply Fourier transform
Y = np.fft.fft(y)

# Plot the frequency content
import matplotlib.pyplot as plt
plt.plot(np.abs(Y))
plt.show()`);o(t,e)},$$slots:{default:!0}});var Y=s(L);u(Y,{children:(t,n)=>{r();var e=a("This code makes a simple sound wave, applies a Fourier transform to it, and plots the amount of each frequency present in the sound.");o(t,e)},$$slots:{default:!0}});var j=s(Y);u(j,{children:(t,n)=>{ve(t,{src:"/images/blog/state-of-audio-processing/fourier.png",alt:"Fourier transform plot"})},$$slots:{default:!0}});var G=s(j);g(G,{level:2,children:(t,n)=>{r();var e=a("Deep neural networks");o(t,e)},$$slots:{default:!0}});var W=s(G);u(W,{children:(t,n)=>{r();var e=a("Deep neural networks (DNNs) are a way to make computers learn to recognize patterns in data, like recognizing different instruments in a piece of music. They work by passing the data through several layers that gradually pick out more and more complex features.");o(t,e)},$$slots:{default:!0}});var O=s(W);u(O,{children:(t,n)=>{r();var e=a("For example, to train a DNN to recognize musical instruments:");o(t,e)},$$slots:{default:!0}});var E=s(O);w(E,{ordered:!0,marker:".",children:(t,n)=>{var e=Ne(),l=f(e);v(l,{children:(i,d)=>{r();var h=a("You give it many recordings of each instrument");o(i,h)},$$slots:{default:!0}});var c=s(l);v(c,{children:(i,d)=>{r();var h=a("The DNN learns patterns unique to each instrument");o(i,h)},$$slots:{default:!0}});var p=s(c);v(p,{children:(i,d)=>{r();var h=a("After training, the DNN can recognize the instrument in new recordings it hasn't heard before");o(i,h)},$$slots:{default:!0}}),o(t,e)},$$slots:{default:!0}});var J=s(E);u(J,{children:(t,n)=>{r();var e=a("Here's a simple DNN in code:");o(t,e)},$$slots:{default:!0}});var K=s(J);le(K,{content:`import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(500,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_data, train_labels, epochs=10, batch_size=32)
`,language:"python",process:!0,children:(t,n)=>{r();var e=a(`import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(500,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_data, train_labels, epochs=10, batch_size=32)`);o(t,e)},$$slots:{default:!0}});var B=s(K);u(B,{children:(t,n)=>{r();var e=a("This code makes a simple DNN. It expects 500 input features (which could be the frequency content of a sound clip). It has two layers that process the data, and an output layer that predicts which of 10 possible instruments the sound is.");o(t,e)},$$slots:{default:!0}});var Z=s(B);u(Z,{children:(t,n)=>{r();var e=Ae(),l=s(f(e));_e(l,{content:"fit()"}),r(),o(t,e)},$$slots:{default:!0}});var Q=s(Z);u(Q,{children:(t,n)=>{r();var e=a("DNNs are useful in audio for things like:");o(t,e)},$$slots:{default:!0}});var U=s(Q);w(U,{ordered:!1,marker:"-",children:(t,n)=>{var e=Te(),l=f(e);v(l,{children:(d,h)=>{r();var m=a("Converting speech to text");o(d,m)},$$slots:{default:!0}});var c=s(l);v(c,{children:(d,h)=>{r();var m=a("Identifying the genre of a piece of music");o(d,m)},$$slots:{default:!0}});var p=s(c);v(p,{children:(d,h)=>{r();var m=a("Detecting emotions in speech");o(d,m)},$$slots:{default:!0}});var i=s(p);v(i,{children:(d,h)=>{r();var m=a("Removing background noise from recordings");o(d,m)},$$slots:{default:!0}}),o(t,e)},$$slots:{default:!0}});var V=s(U);u(V,{children:(t,n)=>{r();var e=a("The ability of DNNs to learn complex patterns makes them very useful for modern audio processing tasks.");o(t,e)},$$slots:{default:!0}});var X=s(V);g(X,{level:2,children:(t,n)=>{r();var e=a("Automatic speech recognition (ASR)");o(t,e)},$$slots:{default:!0}});var ee=s(X);u(ee,{children:(t,n)=>{r();var e=a("ASR systems convert spoken words into written text. They're incredibly useful for dictating messages, giving voice commands, or automatically transcribing meetings or lectures. ASR has improved dramatically thanks to deep learning, which allows these systems to learn patterns from vast amounts of speech data.");o(t,e)},$$slots:{default:!0}});var te=s(ee);u(te,{children:(t,n)=>{r();var e=a("One of the best ASR systems is DeepSpeech, developed by Mozilla. It uses a deep neural network architecture called a recurrent neural network (RNN), which is particularly good at handling sequential data like speech. DeepSpeech 2, released in 2017, achieved a word error rate of just 5.5% on a standard benchmark, which was a significant milestone. Google's recent Conformer model has pushed the state of the art even further.");o(t,e)},$$slots:{default:!0}});var re=s(te);g(re,{level:2,children:(t,n)=>{r();var e=a("Sound generation and music composition");o(t,e)},$$slots:{default:!0}});var oe=s(re);u(oe,{children:(t,n)=>{r();var e=a("Generative AI models can create new sounds and music by learning patterns from existing audio data. This opens up exciting possibilities for music creation, sound design, and personalized audio content.");o(t,e)},$$slots:{default:!0}});var se=s(oe);u(se,{children:(t,n)=>{var e=Me(),l=f(e);_(l,{href:"https://arxiv.org/abs/1609.03499",children:(p,i)=>{r();var d=a("WaveNet, developed by DeepMind in 2016");o(p,d)},$$slots:{default:!0}});var c=s(l,2);_(c,{href:"https://arxiv.org/abs/1712.05884",children:(p,i)=>{r();var d=a("Google's Tacotron 2");o(p,d)},$$slots:{default:!0}}),r(),o(t,e)},$$slots:{default:!0}});var ae=s(se);u(ae,{children:(t,n)=>{r();var e=De(),l=s(f(e));_(l,{href:"https://openai.com/blog/musenet",children:(p,i)=>{r();var d=a("OpenAI's MuseNet");o(p,d)},$$slots:{default:!0}});var c=s(l,2);_(c,{href:"https://arxiv.org/abs/2005.00341",children:(p,i)=>{r();var d=a("Jukebox");o(p,d)},$$slots:{default:!0}}),r(),o(t,e)},$$slots:{default:!0}});var ne=s(ae);g(ne,{level:1,children:(t,n)=>{r();var e=a("Get started with audio processing in Backrush");o(t,e)},$$slots:{default:!0}});var ie=s(ne);u(ie,{children:(t,n)=>{r();var e=a("Backrush provides a platform to build your own audio processing applications. With our AI function templates and tutorials, you can quickly get started with voice recognition, music generation, and more.");o(t,e)},$$slots:{default:!0}});var pe=s(ie);me(pe,{children:(t,n)=>{var e=Se(),l=f(e);P(l,{href:"/docs/products/ai/tutorials/speech-recognition",title:"Speech recongition",children:(i,d)=>{u(i,{children:(h,m)=>{r();var y=a("Efficiently convert speech to text");o(h,y)},$$slots:{default:!0}})},$$slots:{default:!0}});var c=s(l);P(c,{href:"/docs/products/ai/tutorials/text-to-speech",title:"Text to speech",children:(i,d)=>{u(i,{children:(h,m)=>{r();var y=a("Generate lifelike speech from text");o(h,y)},$$slots:{default:!0}})},$$slots:{default:!0}});var p=s(c);P(p,{href:"/docs/products/ai/tutorials/music-generation",title:"Music generation",children:(i,d)=>{u(i,{children:(h,m)=>{r();var y=a("Create unique musical compositions");o(h,y)},$$slots:{default:!0}})},$$slots:{default:!0}}),o(t,e)}}),$e(x),o(ce,x)},$$slots:{default:!0}}))}const ot=Object.freeze(Object.defineProperty({__proto__:null,default:Ie,frontmatter:de},Symbol.toStringTag,{value:"Module"}));export{ot as _,Ie as a};
